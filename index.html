<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Incorporating Foundation Model Priors in Modeling Novel Objects for Robot Instruction Following in Unstructured Environments">
  <meta name="keywords" content="3D Model Reconstruction, Guided Filtering, Foundation Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>3D-Object-Modelling</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <header class="header">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Incorporating Foundation Model Priors in Modeling Novel Objects for Robot Instruction Following in Unstructured Environments</h1>
              <!-- h3 class="title is-4 conference-authors" style="color: rgb(0, 102, 255);">......</h3 -->
              <div class="is-size-5 publication-authors" , style="margin-bottom: 5mm;">
                <span class="author-block">
                  Moksh Malhotra<sup>1</sup>,</span>
                <span class="author-block">
                  Aman Tambi<sup>1*</sup>,</span>
                <span class="author-block">
                  Sandeep S. Zachariah<sup>1*</sup>,</span>
                <span class="author-block">
                  P. V. M. Rao<sup>1</sup>,</span>
                <span class="author-block">
                  Rohan Paul<sup>1</sup></span>
              </div>

              <div class="is-size-6 publication-authors" , style="margin-bottom: 8mm;">
                <span class="author-block"><sup>1</sup>Affiliated with Indian Institute of Technology Delhi,</span>
                <span class="author-block"><sup>{*}</sup>Indicate equal contributions, </span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.15767.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->
                  <!-- <span class="link-block">
                    <a href="http://arxiv.org/abs/2402.15767" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span> -->
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmgQnyyezjnUCMDo3N?e=54ufNt"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/phyplan/PhyPlan"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span> -->
                  <!-- Code Link. -->
                  <!-- <span class="link-block">
                    <a href="https://1drv.ms/f/s!AjIGOzSicYxmcmRXYJJINf42I20?e=XhDGne"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </header>
  </section>

  


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <h2 class="title has-text-centered is-3">Abstract</h2>
      <div class="columns is-centered is-vcentered" , padding-left: 7%;">
        <!-- Image Column -->
        <div style="text-align: center;">
          <img src="./static/images/naive_vs_our_approach_motivation_v2.jpg"  style="max-width: 70%; margin: 0 auto;" />
        </div>
        <!-- Text Column -->
        </div>
      <div class="content is-four-fifths has-text-justified">
        <p>
          This paper addresses the challenge of acquiring object models to 
          facilitate task execution in unstructured, unknown environments. 
          Illustrated through the scenario of commanding a robot to explore 
          unfamiliar terrain and interact with objects, the necessity of a 
          metric-semantic representation of the environment becomes apparent. 
          Such representation not only identifies present objects but also 
          maintains their geometric attributes for future interactions, 
          such as manipulation or relocation. Handling large, unfamiliar objects, 
          like trusses or tree branches, necessitates intricate reasoning in 
          grasping, transport, and placement phases, posing a significant 
          challenge for a versatile manipulation agent. The paper proposes 
          an approach that integrates prior knowledge from pre-trained models 
          with real-time data to generate detailed object models, essential 
          for sequential manipulation tasks. This method involves utilizing 
          pre-trained Vision-and-Language Models (VLMs) to extract object masks 
          from raw point clouds and integrating depth priors from foundation models 
          for improved geometric accuracy. Furthermore, the approach includes 
          mechanisms for building local maps and local repairs over sequential 
          action execution. Experimental results demonstrate the effectiveness 
          of the proposed approach in acquiring high-quality 3D object models 
          compared to alternative methods for unstructured scenarios.
        </p>
      </div>
      
      </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3 has-text-centered">Approach Overview</h2>
        <div class="columns is-centered is-vcentered" , padding-left: 7%;">
          <div class="content is-four-fifths has-text-justified"></div>
            <p>
              The proposed method sequentially builds the global point cloud 
              using a sequence of posed RGBD images. A guided filter employing 
              depth priors from foundation models is used to refine the noisy 
              depth images. The semantic extractor detects all objects and 
              generates the segmentation mask for each object, which is fused 
              with the global point cloud to extract the 3D model of the objects.
            </p>
          </div>
          <!-- Image Column -->
          <div>
            <img src="./static/images/methodology_v2.jpg"
              />
          </div>
          <!-- Text Column -->
          </div>
        </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Results</h2>
      <h3 class="title is-4">3D Object Models</h3>

      <div class="container is-max-desktop">
        <div class="columns">
          <!-- First Column -->
          <div class="column">
            <h3 class="subtitle is-4">Qualitative Comparision</h3>
            <div class="image-container">
              <img src="./static/images/3D_models _Qualitative_analysis.png" alt="Image 1">
            </div>
            <div class="text-container">
              <p>he proposed
                method, which utilizes depth priors, produces more accurate
                and complete 3D object models in relation to the unfiltered
                approach. Specifically, the resulting models show lower noise
                reduction, smooth surfaces, and capture structural informa-
                tion, which is lacking in the unfiltered method (directly masking raw point cloud).
                (Ignore the colors)</p>
            </div>
          </div>

          <!-- Second Column -->
          <div class="column">
            <h3 class="subtitle is-4">Quantitative Analysis</h3>
            <div class="image-container">
              <img src="./static/images/Accuracy_n_RMSE.png" alt="Image 2">
            </div>
            <div class="text-container">
              <p>We quantitatively evaluate our method using two metrics:
                (i) Root Mean Squared Error (RMSE): RMSE quantifies the
                root mean squared distance of each point in the reconstructed
                model to the nearest point in the ground truth. Results indi-
                cate that the proposed approach has a lower error compared
                to the unfiltered approach with respect to the ground truth.
                (ii) Model Reconstruction Accuracy (MRA): MRA quantifies
                the percentage of points in the reconstructed model that
                are within a distance d to the nearest point in the ground
                truth. Results indicate that the proposed method has a higher
                Model Reconstruction Accuracy compared to the unfiltered
                approach, meaning that a higher percentage of points of the
                reconstructed model are within tolerance d from the ground
                truth.</p>
            </div>
          </div>
        </div>
      </div>
      
      <br>
      <br>
      <h3 class="title is-4">Local Scene Update Post Action Execution</h3>
      <br>
      <div class="columns is-centered is-vcentered" , padding-left: 7%;">
        <!-- Image Column -->
        <div>
          <img src="./static/images/3d_construction_plan_rollout.drawio_compress.png"
            />
        </div>
        <!-- Text Column -->
        </div>
        <div class="content is-four-fifths has-text-justified"></div>
          <p>
            Visualization of the plan rollout and scene reconstruction for a scenario involving occlusion. Red arrows indicate the pose update of the object,
            while blue arrows represent local scene reconstruction. Initially, only the briefcase was visible. Upon removing the briefcase and subsequent local rebuilding,
            the hose was detected. The figure also demonstrates how the scene is updated when the object is manipulated without requiring global scene reconstruction.
          </p>
        </div>
      </div>
    </div>
  </section>

  <hr>

  <!-- <section class="section" id="References">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <pre><code>1. [Allen et al., 2020] Kelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum.
        Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning.
        Proceedings of the National Academy of Sciences, 117(47):29302â€“29310, 2020.</code></pre>
      <pre><code>2. [Bakhtin et al., 2019] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick.
        Phyre: A new benchmark for physical reasoning.
        Advances in Neural Information Processing Systems, 32,484 2019.</code></pre>
    </div>
  </section> -->

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2>
      <pre><code>@inproceedings{phyplan2024,
      title     = {PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators},
      author    = {Vagadia, Harshil and Chopra, Mudit and Barnawal, Abhinav and Banerjee, Tamajit and Tuli, Shreshth and Chakraborty, Souvik and Paul, Rohan},
      booktitle = {},
      year      = {2024}
    }</code></pre>
    </div>
  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
      </div>
    </div>
  </footer>

</body>

</html>
